
library(devtools)
library(dada2)
library(ggplot2)
library(dada2); packageVersion("dada2")





path_2019_ITS_Fungi <- "/data/home/christian.andersen/ITS_LGC_CBA/PC_2019_ITS_PGP/"
path_2019_ITS_Fungi
list.files(path_2019_ITS_Fungi)


fnFs_2019_ITS_Fungi <- sort(list.files(path_2019_ITS_Fungi, pattern="_R1.fastq.bz2", full.names = TRUE))
fnRs_2019_ITS_Fungi <- sort(list.files(path_2019_ITS_Fungi, pattern="_R2.fastq.bz2", full.names = TRUE))
fnFs_2019_ITS_Fungi

sample.names <- sapply(strsplit(basename(fnFs_2019_ITS_Fungi), ".fastq.bz2"), `[`, 1)
sample.names


filtFs_2019_ITS_Fungi <- file.path(path_2019_ITS_Fungi, "filtered", paste0(sample.names, "_R1_filt.fastq.bz2"))
filtRs_2019_ITS_Fungi <- file.path(path_2019_ITS_Fungi, "filtered", paste0(sample.names, "_R2_filt.fastq.bz2"))
names(filtFs_2019_ITS_Fungi) <- sample.names
names(filtRs_2019_ITS_Fungi) <- sample.names


# learn the error rates of the reads# 
errF_2019_ITS_Fungi <- learnErrors(filtFs_2019_ITS_Fungi, multithread=TRUE)
errR_2019_ITS_Fungi <- learnErrors(filtRs_2019_ITS_Fungi, multithread=TRUE)


derepFs_2019_ITS_Fungi <- derepFastq(filtFs_2019_ITS_Fungi, verbose = TRUE)
derepRs_2019_ITS_Fungi <- derepFastq(filtRs_2019_ITS_Fungi, verbose = TRUE)
# Name the derep-class objects by the sample names
names(derepFs_2019_ITS_Fungi) <- sample.names
names(derepRs_2019_ITS_Fungi) <- sample.names



# now we are ready for the core algorhims of the dada2 pipeline, 
dadaFs_2019_ITS_Fungi<- dada(filtFs_2019_ITS_Fungi, err=errF_2019_ITS_Fungi, multithread=TRUE)
dadaRs_2019_ITS_Fungi <- dada(filtRs_2019_ITS_Fungi, err=errR_2019_ITS_Fungi, multithread=TRUE)

mergers_2019_ITS_Fungi <- mergePairs(dadaFs_2019_ITS_Fungi, derepFs_2019_ITS_Fungi, dadaRs_2019_ITS_Fungi, derepRs_2019_ITS_Fungi, verbose=TRUE)


head(mergers_2019_ITS_Fungi[[1]])

seqtab_2019_ITS_Fungi <- makeSequenceTable(mergers_2019_ITS_Fungi)
dim(seqtab_2019_ITS_Fungi)

table(nchar(getSequences(seqtab_2019_ITS_Fungi)))


seqtab.nochim_2019_ITS_Fungi <- removeBimeraDenovo(seqtab_2019_ITS_Fungi, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim_2019_ITS_Fungi)

sum(seqtab.nochim_2019_ITS_Fungi)/sum(seqtab_2019_ITS_Fungi)

## read out file## 


out_2019_ITS_Fungi <- read.csv("/data/home/christian.andersen/ITS_LGC_CBA/PC_2019_ITS_PGP/out/out_2019_ITS_Final.csv", row=1)
out_2019_ITS_Fungi

getNout_2019_ITS_Fungi <- function(x) sum(getUniques(x))
trackout_2019_ITS_Fungi <- cbind(out_2019_ITS_Fungi, sapply(dadaFs_2019_ITS_Fungi, getNout_2019_ITS_Fungi), sapply(dadaRs_2019_ITS_Fungi, getNout_2019_ITS_Fungi), sapply(mergers_2019_ITS_Fungi, getNout_2019_ITS_Fungi), rowSums(seqtab.nochim_2019_ITS_Fungi))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(trackout_2019_ITS_Fungi) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(trackout_2019_ITS_Fungi) <- sample.names
head(trackout_2019_ITS_Fungi)

taxa_2019_ITS_Fungi <- assignTaxonomy(seqtab.nochim_2019_ITS_Fungi, "/data/home/christian.andersen/sh_general_release_dynamic_all_18.07.2023.fasta")
otu_2019_ITS_Fungi <- t(seqtab.nochim_2019_ITS_Fungi)
write.csv(otu_2019_ITS_Fungi, file="otu_2019_ITS_Fungi.csv")
write.csv(taxa_2019_ITS_Fungi, file="taxa_2019_ITS_Fungi.csv")